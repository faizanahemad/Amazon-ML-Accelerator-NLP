{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "![MLA Logo](https://drive.corp.amazon.com/view/mrruckma@/MLA_headerv2.png?download=true)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Let's ensure we're using Eider's default compute with the Credential cell below:" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import pandas as pd\neider.s3.download(\"s3://eider-datasets/mlu/projects/MLANLPIFinalProject/training.csv\", \"/tmp/training.csv\")" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df = pd.read_csv('/tmp/training.csv', encoding='utf-8', header=0)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Let's take a look at this data in more detail and then start working. Remember 'human_tag' is our target variable/column\ndf.head(5)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# __1-Pre-processing Training Data:__\n* Read the data\n* Remove nan values\n* Remove stopwords and apply stemming\n\nLet's check our data for nan values. We want to remove rows with nan values in one or more columns." ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Let's see how many nan values in our data frame\nprint(df.isna().sum())\n\n# Let's remove them\ndf.dropna(inplace=True)\nprint(\"------------nan rows removed!-------------\")\n\n# Let's see how many nan values in our data frame\nprint(df.isna().sum())" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# __2-Splitting the training dataset into training and validation__\n* Features: Title, text, star_rating\n* Target: human_tag" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(df[[\"title\", \"text\", \"star_rating\"]], df[\"human_tag\"].values, test_size=0.1, shuffle=True)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# __3-Stop Word Removal and Stemming__" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt', download_dir='/tmp/')\nnltk.download('stopwords', download_dir='/tmp/')\nnltk.data.path.append(\"tmp\")\n\nsnow = SnowballStemmer('english') \nstop = stopwords.words('english')\n\n#excluding some useful words from stop words list\nexcluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n            \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n            \nstop_words = [word for word in stop if word not in excluding]\n\ndef process_text(texts): \n    final_text_list=[]\n    for sent in texts:\n        filtered_sentence=[]\n        sent = sent.lower()\n        for w in word_tokenize(sent):\n            # Check if it is not numeric and its length>2 and not in stop words\n            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n                # Stem and add to filtered list\n                filtered_sentence.append(snow.stem(w))\n        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n\n        final_text_list.append(final_string)\n    return final_text_list" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "__Let's process text and title fields__" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(\"Pre-process training dataset\")\nX_train_title_processed = process_text(X_train[\"title\"].values) \nX_train_text_processed = process_text(X_train[\"text\"].values) \n\nprint(\"Pre-process validation dataset\")\nX_val_title_processed = process_text(X_val[\"title\"].values) \nX_val_text_processed = process_text(X_val[\"text\"].values) " ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# __4-Computing TF-IDF Vectors__\n* Use title, text and star_rating columns.\n* We will compute the TF-IDF vectors for training, validation and test separately." ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "Let's calculate TF-IDF features for training and validation" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_title_vectorizer = TfidfVectorizer(max_features=3500,  ngram_range=(1, 2))\ntfidf_text_vectorizer = TfidfVectorizer(max_features=9000, ngram_range=(1, 2)) # Text field is much longer than title\n\n# We fit the vectorizers using only the training data\ntfidf_title_vectorizer.fit(X_train_title_processed)\ntfidf_text_vectorizer.fit(X_train_text_processed)\n\nX_train_title_vectors = tfidf_title_vectorizer.transform(X_train_title_processed)\nX_train_text_vectors = tfidf_text_vectorizer.transform(X_train_text_processed)\n\nX_val_title_vectors = tfidf_title_vectorizer.transform(X_val_title_processed)\nX_val_text_vectors = tfidf_text_vectorizer.transform(X_val_text_processed)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "Let's put everything together: star rating + title vector + text vector\n\nThe size of feature for each row becomes 1 + 3500 + 9000 = 12501" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import numpy as np\n\n# ------Training data -----------\n# Normalize star rating\nX_train_star_rating_norm = X_train[\"star_rating\"] / X_train[\"star_rating\"].max()\n\n# Let's use the star rating and the tf-idf vectors together \nX_train_merged = np.column_stack((X_train_star_rating_norm, X_train_title_vectors.toarray(), X_train_text_vectors.toarray()))\n\n# ------Validation data -----------\n# Normalize star rating\nX_val_star_rating_norm = X_val[\"star_rating\"] / X_val[\"star_rating\"].max()\n\n# Let's use the star rating and the tf-idf vectors together \nX_val_merged = np.column_stack((X_val_star_rating_norm, X_val_title_vectors.toarray(), X_val_text_vectors.toarray()))\n" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# __5-Training:__\n* Train using X_train and y_train\n* Use validation data (X_val and y_val) to see how well it works." ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "#Training the model and Testing Accuracy on Validation data\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, f1_score\n\n# Try l1 and l2 regularization with coeff [0.05, 0.1, ..., 1.7, 1.75]\nparameters = {'penalty':['l1', 'l2'], 'C': np.arange(0.05, 1.75, 0.05)}\nlr = LogisticRegression(class_weight='balanced')\n\n# You can experiment with different cv numbers.\nclf = RandomizedSearchCV(lr, parameters, cv=5, scoring=make_scorer(f1_score), verbose=1, n_jobs=-1)\n\nclf.fit(X_train_merged, y_train)\n\ny_val_pred = clf.predict(X_val_merged)\ny_val_pred_probs = clf.predict_proba(X_val_merged)\nprint(classification_report(y_val, y_val_pred)) " ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# __6-Picking the Probability Threshold__:\nWe will plot Precision-Recall curve and pick the point with the highest f1 score. We can easliy calculate f1 score using precision and recall." ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_val, y_val_pred_probs[:, 1])\n\n# Let's plot the Precision-Recall curve, precision on the y axis and recall on the x axis\n\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n\n# plot the roc curve for the model\nplt.plot(recalls, precisions, marker='.')\n\nplt.title('Precision-Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\n# show the plot\nplt.show()" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "We will calculate the F1 score using the precision and recall from the curve above. Later, we will pick the threshold that resulted in the largest F1 score as our new decision boundary.\n\n![f1](https://drive-render.corp.amazon.com/view/cesazara@/cv-notebook-images/f1_score.png?download=true)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "highest_f1 = 0\nthreshold_highest_f1 = 0\nfor id, threhold in enumerate(thresholds):\n    f1_score = 2*precisions[id]*recalls[id]/(precisions[id]+recalls[id])\n    if(f1_score > highest_f1):\n        highest_f1 = f1_score\n        threshold_highest_f1 = threhold\nprint(\"Highest F1:\", highest_f1, \", Threshold for the highest F1:\", threshold_highest_f1)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# __7-Getting predictions on test data and saving results__:\nPrepare the test data using the previous same steps (except splitting)\n* Read the test data and impute mising values\n* Stopword removal and stemming\n* Get TF-IDF feaures\n* Predict the results using the same model we trained on. \n* Save the result to tmp folder" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "__Reading and imputing:__" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Download the test data\neider.s3.download(\"s3://eider-datasets/mlu/projects/MLANLPIFinalProject/public_test_features.csv\", \"/tmp/public_test_features.csv\")\n\n# Read the test data (It doesn't have the human_tag label, we are trying to predict that :D )\ntest_df = pd.read_csv('/tmp/public_test_features.csv', encoding='utf-8', header=0)\n\n# Fill missing text and title values\ntest_df['text'] = test_df['text'].fillna(\"\")\ntest_df['title'] = test_df['title'].fillna(\"\")" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "__Stopword removal and stemming:__" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(\"Process test dataset\")\nX_test_title_processed = process_text(test_df[\"title\"].values) \nX_test_text_processed = process_text(test_df[\"text\"].values) " ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "__Get TF-IDF feaures:__" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "X_test_title_vectors = tfidf_title_vectorizer.transform(X_test_title_processed)\nX_test_text_vectors = tfidf_text_vectorizer.transform(X_test_text_processed)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "__Predict the results using the same model we trained on:__" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# ------Test data -----------\n\n# Normalize star rating\nX_test_star_rating_norm = test_df[\"star_rating\"] / test_df[\"star_rating\"].max()\n\n# Let's use the star rating and the tf-idf vectors together \nX_test_merged = np.column_stack((X_test_star_rating_norm, X_test_title_vectors.toarray(), X_test_text_vectors.toarray()))\n\n# Make predictions using our trained model\ntest_prediction_probs = clf.predict_proba(X_test_merged)[:, 1]\n\n# Let's apply the new threshold to this value\ntest_prediction = np.where(test_prediction_probs > threshold_highest_f1, 1, 0)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "__Save the result to tmp folder__" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import pandas as pd\n\nresult_df = pd.DataFrame()\nresult_df[\"ID\"] = test_df[\"ID\"]\nresult_df[\"human_tag\"] = test_prediction\n\nresult_df.to_csv(\"tmp/project_day2_result.csv\", encoding='utf-8', index=False)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# 7-Getting our model output out of Eider and into Leaderboard\nGreat. Now we have a dummie sample submission in Eider that we now need to export locally so that we may then upload to Leaderboard in the following steps:\n1. Within the Eider console top bar, select [Files](https://eider.corp.amazon.com/file)\n2. You should now see 'Files', 'TMP' and 'Exported notebooks' tabs. \n3. Select 'TMP' then select 'Connect to workspace'. You should now see any files from your last run of your workspace. If there was no 'Connect to workspace' option, your files from the last run should already be present.\n4. Go to the 'my_sample_output_day2.csv' file and select Save\n5. This file will now be permanently saved to your Eider account and available for local download from the 'Files' tab via the download button.\n\nWe now have our model's output .csv and are ready to upload to Leaderboard\n1. Search for your class [Leaderboard instance](https://leaderboard.corp.amazon.com/) and go to the 'Make a Submission' section\n2. Upload your local file and include your notebook version URL for tracking\n3. Your score on the public leaderboard should now appear. Marvel on how much room for improvement there is\n" ]
  } ]
}